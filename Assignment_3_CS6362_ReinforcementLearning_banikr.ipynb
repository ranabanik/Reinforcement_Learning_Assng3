{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning on Markov Decision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries of Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from anytree import Node\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Create class for the MDP model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State():\n",
    "    def __init__(self, name, actions, next_states, probs, rewards):\n",
    "        self.name = name\n",
    "        self.actions = actions\n",
    "        self.next_states = next_states\n",
    "        self.probs = probs\n",
    "        self.rewards = rewards\n",
    "        if len(self.actions) == 0:\n",
    "            self.isendstate = True\n",
    "        else:\n",
    "            self.isendstate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Creating the data structure of the MDP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending = State('11a', [], [], [], [])\n",
    "TU10a = State('TU10a', ['P', 'R', 'S'], [ending, ending, ending], [1., 1., 1.], [-1., -1., -1.])\n",
    "RU10a = State('RU10a', ['P', 'R', 'S'], [ending, ending, ending], [1., 1., 1.], [0., 0., 0.])\n",
    "RD10a = State('RD10a', ['P', 'R', 'S'], [ending, ending, ending], [1., 1., 1.], [4., 4., 4.])\n",
    "TD10a = State('TD10a', ['P', 'R', 'S'], [ending, ending, ending], [1., 1., 1.], [3., 3., 3.])\n",
    "RU8a = State('RU8a', ['P', 'R', 'S'], [TU10a, RU10a, RD10a], [1., 1., 1.], [2., 0., -1.])\n",
    "RD8a = State('RD8a', ['P', 'R'], [TD10a, RD10a], [1., 1.], [2., 0.])\n",
    "TU10p = State('TU10p', ['P', 'R'], [RU10a, RU8a], [1., 1.], [2., 0.])\n",
    "RU10p = State('RU10p', ['P', 'P', 'R', 'S'], [RU8a, RU10a, RU8a, RD8a], [0.5, 0.5, 1., 1.], [2., 2., 0., -1.])\n",
    "RD10p = State('RD10p', ['P', 'P', 'R'], [RD8a, RD10a, RD8a], [0.5, 0.5, 1.], [2., 2., 0.])\n",
    "RU8p = State('RU8p', ['P', 'R', 'S'], [TU10p, RU10p, RD10p], [1., 1., 1.], [2., 0., -1.])\n",
    "\n",
    "states = [RU8p, TU10p, RU10p, RD10p, RU8a, RD8a, TU10a, RU10a, RD10a, TD10a, ending]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The functions below prints the state properties: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_names(states):\n",
    "    State_list = []\n",
    "    for i in states:\n",
    "        State_list.append(i.name)\n",
    "    return State_list\n",
    "\n",
    "def get_policy_actions_or_values(Policy): #or Vinit\n",
    "    Action_list = []\n",
    "    for i in Policy:\n",
    "        Action_list.append(Policy[i])\n",
    "    return Action_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes for 3a:\n",
    "\n",
    "> The following part prints out the ordered sequence of states/actions/rewards for 50 episodes:<br>\n",
    "to be noted that the average return for 50 episodes vary in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0                           1             2                      3  \\\n",
      "0   #Episode                      States       Actions                Rewards   \n",
      "1          1  [RU8p, TU10p, RU8a, RD10a]  [P, R, S, P]  [2.0, 0.0, -1.0, 4.0]   \n",
      "2          2  [RU8p, TU10p, RU8a, RU10a]  [P, R, R, R]   [2.0, 0.0, 0.0, 0.0]   \n",
      "3          3  [RU8p, TU10p, RU8a, RD10a]  [P, R, S, S]  [2.0, 0.0, -1.0, 4.0]   \n",
      "4          4  [RU8p, RD10p, RD8a, TD10a]  [S, P, P, R]  [-1.0, 2.0, 2.0, 3.0]   \n",
      "5          5  [RU8p, RU10p, RU8a, TU10a]  [R, R, P, R]  [0.0, 0.0, 2.0, -1.0]   \n",
      "6          6        [RU8p, TU10p, RU10a]     [P, P, P]        [2.0, 2.0, 0.0]   \n",
      "7          7        [RU8p, RD10p, RD10a]     [S, P, R]       [-1.0, 2.0, 4.0]   \n",
      "8          8  [RU8p, TU10p, RU8a, RD10a]  [P, R, S, P]  [2.0, 0.0, -1.0, 4.0]   \n",
      "9          9  [RU8p, RD10p, RD8a, TD10a]  [S, R, P, R]  [-1.0, 0.0, 2.0, 3.0]   \n",
      "10        10  [RU8p, RD10p, RD8a, TD10a]  [S, P, P, P]  [-1.0, 2.0, 2.0, 3.0]   \n",
      "11        11  [RU8p, RD10p, RD8a, RD10a]  [S, P, R, R]  [-1.0, 2.0, 0.0, 4.0]   \n",
      "12        12        [RU8p, TU10p, RU10a]     [P, P, P]        [2.0, 2.0, 0.0]   \n",
      "13        13        [RU8p, TU10p, RU10a]     [P, P, P]        [2.0, 2.0, 0.0]   \n",
      "14        14  [RU8p, TU10p, RU8a, RD10a]  [P, R, S, P]  [2.0, 0.0, -1.0, 4.0]   \n",
      "15        15  [RU8p, TU10p, RU8a, TU10a]  [P, R, P, R]  [2.0, 0.0, 2.0, -1.0]   \n",
      "16        16  [RU8p, RU10p, RU8a, TU10a]  [R, P, P, R]  [0.0, 2.0, 2.0, -1.0]   \n",
      "17        17  [RU8p, RU10p, RD8a, RD10a]  [R, S, R, P]  [0.0, -1.0, 0.0, 4.0]   \n",
      "18        18        [RU8p, TU10p, RU10a]     [P, P, R]        [2.0, 2.0, 0.0]   \n",
      "19        19  [RU8p, RU10p, RU8a, TU10a]  [R, P, P, P]  [0.0, 2.0, 2.0, -1.0]   \n",
      "20        20  [RU8p, RD10p, RD8a, RD10a]  [S, P, R, R]  [-1.0, 2.0, 0.0, 4.0]   \n",
      "21        21        [RU8p, RD10p, RD10a]     [S, P, R]       [-1.0, 2.0, 4.0]   \n",
      "22        22  [RU8p, RU10p, RU8a, RU10a]  [R, R, R, P]   [0.0, 0.0, 0.0, 0.0]   \n",
      "23        23        [RU8p, TU10p, RU10a]     [P, P, R]        [2.0, 2.0, 0.0]   \n",
      "24        24  [RU8p, RU10p, RU8a, RD10a]  [R, P, S, R]  [0.0, 2.0, -1.0, 4.0]   \n",
      "25        25  [RU8p, TU10p, RU8a, RD10a]  [P, R, S, R]  [2.0, 0.0, -1.0, 4.0]   \n",
      "26        26  [RU8p, TU10p, RU8a, RD10a]  [P, R, S, R]  [2.0, 0.0, -1.0, 4.0]   \n",
      "27        27  [RU8p, RD10p, RD8a, RD10a]  [S, R, R, S]  [-1.0, 0.0, 0.0, 4.0]   \n",
      "28        28  [RU8p, TU10p, RU8a, TU10a]  [P, R, P, R]  [2.0, 0.0, 2.0, -1.0]   \n",
      "29        29        [RU8p, RD10p, RD10a]     [S, P, R]       [-1.0, 2.0, 4.0]   \n",
      "30        30        [RU8p, RU10p, RU10a]     [R, P, S]        [0.0, 2.0, 0.0]   \n",
      "31        31  [RU8p, RD10p, RD8a, TD10a]  [S, P, P, S]  [-1.0, 2.0, 2.0, 3.0]   \n",
      "32        32  [RU8p, RU10p, RU8a, RD10a]  [R, R, S, S]  [0.0, 0.0, -1.0, 4.0]   \n",
      "33        33  [RU8p, TU10p, RU8a, TU10a]  [P, R, P, R]  [2.0, 0.0, 2.0, -1.0]   \n",
      "34        34        [RU8p, RU10p, RU10a]     [R, P, P]        [0.0, 2.0, 0.0]   \n",
      "35        35  [RU8p, RU10p, RD8a, RD10a]  [R, S, R, S]  [0.0, -1.0, 0.0, 4.0]   \n",
      "36        36        [RU8p, TU10p, RU10a]     [P, P, R]        [2.0, 2.0, 0.0]   \n",
      "37        37  [RU8p, RU10p, RD8a, RD10a]  [R, S, R, S]  [0.0, -1.0, 0.0, 4.0]   \n",
      "38        38  [RU8p, TU10p, RU8a, TU10a]  [P, R, P, S]  [2.0, 0.0, 2.0, -1.0]   \n",
      "39        39  [RU8p, TU10p, RU8a, TU10a]  [P, R, P, R]  [2.0, 0.0, 2.0, -1.0]   \n",
      "40        40        [RU8p, TU10p, RU10a]     [P, P, S]        [2.0, 2.0, 0.0]   \n",
      "41        41  [RU8p, RD10p, RD8a, RD10a]  [S, R, R, S]  [-1.0, 0.0, 0.0, 4.0]   \n",
      "42        42        [RU8p, RD10p, RD10a]     [S, P, P]       [-1.0, 2.0, 4.0]   \n",
      "43        43  [RU8p, RU10p, RD8a, TD10a]  [R, S, P, P]  [0.0, -1.0, 2.0, 3.0]   \n",
      "44        44        [RU8p, RU10p, RU10a]     [R, P, R]        [0.0, 2.0, 0.0]   \n",
      "45        45  [RU8p, RD10p, RD8a, TD10a]  [S, P, P, P]  [-1.0, 2.0, 2.0, 3.0]   \n",
      "46        46        [RU8p, RD10p, RD10a]     [S, P, P]       [-1.0, 2.0, 4.0]   \n",
      "47        47  [RU8p, RD10p, RD8a, TD10a]  [S, P, P, S]  [-1.0, 2.0, 2.0, 3.0]   \n",
      "48        48        [RU8p, RU10p, RU10a]     [R, P, P]        [0.0, 2.0, 0.0]   \n",
      "49        49  [RU8p, RD10p, RD8a, TD10a]  [S, P, P, P]  [-1.0, 2.0, 2.0, 3.0]   \n",
      "50        50  [RU8p, TU10p, RU8a, RU10a]  [P, R, R, S]   [2.0, 0.0, 0.0, 0.0]   \n",
      "\n",
      "         4  \n",
      "0   Return  \n",
      "1        5  \n",
      "2        2  \n",
      "3        5  \n",
      "4        6  \n",
      "5        1  \n",
      "6        4  \n",
      "7        5  \n",
      "8        5  \n",
      "9        4  \n",
      "10       6  \n",
      "11       5  \n",
      "12       4  \n",
      "13       4  \n",
      "14       5  \n",
      "15       3  \n",
      "16       3  \n",
      "17       3  \n",
      "18       4  \n",
      "19       3  \n",
      "20       5  \n",
      "21       5  \n",
      "22       0  \n",
      "23       4  \n",
      "24       5  \n",
      "25       5  \n",
      "26       5  \n",
      "27       3  \n",
      "28       3  \n",
      "29       5  \n",
      "30       2  \n",
      "31       6  \n",
      "32       3  \n",
      "33       3  \n",
      "34       2  \n",
      "35       3  \n",
      "36       4  \n",
      "37       3  \n",
      "38       3  \n",
      "39       3  \n",
      "40       4  \n",
      "41       3  \n",
      "42       5  \n",
      "43       4  \n",
      "44       2  \n",
      "45       6  \n",
      "46       5  \n",
      "47       6  \n",
      "48       2  \n",
      "49       6  \n",
      "50       2  \n",
      "Average return for the 50 episodes is: 3.88\n"
     ]
    }
   ],
   "source": [
    "episodes = 50\n",
    "totsum = 0\n",
    "table = [['#Episode', 'States', 'Actions', 'Rewards', 'Return']]\n",
    "for i in range(episodes):\n",
    "    run = True\n",
    "    line = []\n",
    "    state_list = []\n",
    "    act_list = []\n",
    "    rew_list = []\n",
    "    state = RU8p\n",
    "    while (run):\n",
    "        if state.isendstate == False:\n",
    "            nAct = len(state.actions)  # len(ending.actions) = 0\n",
    "            state_list.append(state.name)\n",
    "            choose = np.int(np.random.randint(0, nAct, 1))\n",
    "            act_list.append(state.actions[choose])\n",
    "            rew_list.append(state.rewards[choose])\n",
    "            state = state.next_states[choose]\n",
    "        else:\n",
    "            run = False\n",
    "    line.append([i+1,state_list,act_list,rew_list,np.sum(rew_list)])\n",
    "    table = np.vstack((table,line))\n",
    "    totsum += np.sum(rew_list)\n",
    "df = pd.DataFrame(table)\n",
    "print(df)\n",
    "# export_CSV =df.to_csv(r'C:\\Users\\ranab\\OneDrive\\PycharmProjects\\RL_assignment\\Episode_table.csv')\n",
    "# print(pd.DataFrame(table))\n",
    "print(\"Average return for the {} episodes is: {}\".format(episodes, totsum / 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following function provides the state values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_value(gamma,state):\n",
    "    nAct = len(state.actions)\n",
    "    state.val = np.zeros(nAct)\n",
    "    uAct = len(set(state.actions)) #will remove duplicate values, just uniques\n",
    "    if nAct == 0:\n",
    "        state.val = 0.\n",
    "    else:\n",
    "        eq_prob = 1.0 / uAct\n",
    "\n",
    "        for i in range(nAct):\n",
    "            if state.name == '11a':\n",
    "                nextVs = 0\n",
    "            else:\n",
    "                nextVs = get_state_value(gamma,state.next_states[i])\n",
    "            state.val[i] = eq_prob * state.probs[i]*(state.rewards[i]+(gamma * nextVs))\n",
    "    return np.sum(state.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0       1                   2\n",
      "0   No.  States              Values\n",
      "1     1    RU8p  3.5138888888888884\n",
      "2     2   TU10p  1.6666666666666665\n",
      "3     3   RU10p                 2.5\n",
      "4     4   RD10p               5.375\n",
      "5     5    RU8a  1.3333333333333333\n",
      "6     6    RD8a                 4.5\n",
      "7     7   TU10a                -1.0\n",
      "8     8   RU10a                 0.0\n",
      "9     9   RD10a                 4.0\n",
      "10   10   TD10a                 3.0\n",
      "11   11     11a                 0.0\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "row = [['No.','States','Values']]\n",
    "for s in states:\n",
    "    i+=1\n",
    "    line = []\n",
    "    value = get_state_value(1.0,s)\n",
    "    line.append([i,s.name,value])\n",
    "    row = np.vstack((row,line))\n",
    "print(pd.DataFrame(row))\n",
    "# df = pd.DataFrame(row)\n",
    "# export_CSV=df.to_csv(r'C:\\Users\\ranab\\OneDrive\\PycharmProjects\\RL_assignment\\state_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes for 3b:\n",
    "> The following function evaluates the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(Policy,Vinit,states):\n",
    "    theta = 0.001\n",
    "    gamma = 1.0\n",
    "    cnd = True\n",
    "    iter = 0\n",
    "    table = np.vstack((get_state_names(states),get_policy_actions_or_values(Policy)))\n",
    "    while(cnd):\n",
    "        iter += 1\n",
    "        lstAc=[]\n",
    "        lstVal=[]\n",
    "        delta = 0.0 #initially change in state value = 0\n",
    "        for s in states:\n",
    "            v = Vinit[s] #initially 0\n",
    "            a = Policy[s] #P type:str\n",
    "            sum = []\n",
    "            for j in range(len(s.actions)):\n",
    "                if s.actions[j] == a: #1p 2p\n",
    "                    sum.append(s.probs[j]*(s.rewards[j]+gamma*Vinit[s.next_states[j]]))\n",
    "            Vinit[s] = np.sum(sum)\n",
    "            delta = max(delta,abs(v-Vinit[s]))\n",
    "        table = np.vstack((table,get_policy_actions_or_values(Vinit)))\n",
    "        cnd = (delta >= theta)\n",
    "#     print(\"Number of iterations required in policy evaluation:\", iter)\n",
    "#     print(pd.DataFrame(table))\n",
    "    df = pd.DataFrame(table)\n",
    "    return Vinit,df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Starting with the initial value arbitrarily from 0 for all the states. Also setting the initial policy to \"Rock & Roll all night and Party every day\" (i.e. policy should choose to party regardless of what state the agent is in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vinit = {RU8p:0, TU10p:0,RU10p:0,RD10p:0,RU8a:0,RD8a:0,TU10a:0,RU10a:0,RD10a:0,TD10a:0,ending:0}\n",
    "Policy = {RU8p:'P', TU10p:'P',RU10p:'P',RD10p:'P',RU8a:'P',RD8a:'P',TU10a:'P',RU10a:'P',RD10a:'P',TD10a:'P',ending:'P'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1      2      3     4     5      6      7      8      9    10\n",
      "0  RU8p  TU10p  RU10p  RD10p  RU8a  RD8a  TU10a  RU10a  RD10a  TD10a  11a\n",
      "1     P      P      P      P     P     P      P      P      P      P    P\n",
      "2   2.0    2.0    2.0    2.0   2.0   2.0   -1.0    0.0    4.0    3.0  0.0\n",
      "3   4.0    2.0    3.0    5.0   1.0   5.0   -1.0    0.0    4.0    3.0  0.0\n",
      "4   4.0    2.0    2.5    6.5   1.0   5.0   -1.0    0.0    4.0    3.0  0.0\n",
      "5   4.0    2.0    2.5    6.5   1.0   5.0   -1.0    0.0    4.0    3.0  0.0\n"
     ]
    }
   ],
   "source": [
    "Vinit,df = Evaluation(Policy,Vinit,states)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now initialized values of the states have changed to a new one. The next part will improve the policy based on policy iteration algorithm<br>\n",
    "_*to get the results run again from the Vinit,Policy section._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1      2      3     4     5      6      7      8      9    10\n",
      "0  RU8p  TU10p  RU10p  RD10p  RU8a  RD8a  TU10a  RU10a  RD10a  TD10a  11a\n",
      "1     S      R      R      P     S     P      P      P      P      P    P\n",
      "2   5.5    3.0    3.0    6.5   3.0   5.0   -1.0    0.0    4.0    3.0  0.0\n",
      "3     S      R      R      P     S     P      P      P      P      P    P\n",
      "4   5.5    3.0    3.0    6.5   3.0   5.0   -1.0    0.0    4.0    3.0  0.0\n",
      "5     S      R      R      P     S     P      P      P      P      P    P\n",
      "6   5.5    3.0    3.0    6.5   3.0   5.0   -1.0    0.0    4.0    3.0  0.0\n"
     ]
    }
   ],
   "source": [
    "policy_stable = True\n",
    "iterations = 0\n",
    "gamma = 1.0\n",
    "proof = 0\n",
    "table = np.vstack((get_state_names(states),get_policy_actions_or_values(Policy),get_policy_actions_or_values(Vinit)))\n",
    "while(policy_stable): #policy is stable\n",
    "    iterations += 1\n",
    "#     print('Iterations:', iterations)\n",
    "    check_action = 0\n",
    "#     print(get_policy_actions_or_values(Policy))\n",
    "#     print(get_policy_actions_or_values(Vinit))\n",
    "    for s in states:\n",
    "        if s.isendstate == False: #not end state\n",
    "            old_action = Policy[s] #this is the starting policy\n",
    "            uAct = len(set(s.actions)) #3\n",
    "            if uAct == len(s.actions):  # no dual actions\n",
    "                tsum = []\n",
    "                for j in range(len(s.actions)):\n",
    "                    # print(\"Singles\",j)\n",
    "                    tsum.append(s.probs[j]*(s.rewards[j]+gamma*Vinit[s.next_states[j]]))\n",
    "                u = -1 #just to find max\n",
    "                maximum = max(tsum)\n",
    "                for ii in tsum:\n",
    "                    u += 1\n",
    "                    if ii == maximum:\n",
    "                        break\n",
    "                new_action = s.actions[u]\n",
    "                if old_action == new_action:\n",
    "                    check_action+=1\n",
    "                else:\n",
    "                    Policy[s] = new_action\n",
    "                    Vinit,_ = Evaluation(Policy, Vinit, states)\n",
    "            else: #dual actions\n",
    "                ttsum = np.zeros(uAct)\n",
    "                dual_sum = []\n",
    "                for j in range(len(s.actions)):\n",
    "                    if j<2:\n",
    "                        dual_sum.append(s.probs[j]*(s.rewards[j]+gamma*Vinit[s.next_states[j]]))\n",
    "                        d = 0\n",
    "                        ttsum[d] = np.sum(dual_sum)\n",
    "                    else:\n",
    "                        d+=1\n",
    "                        ttsum[d] = s.probs[j]*(s.rewards[j]+gamma*Vinit[s.next_states[j]])\n",
    "                u = -1  #to find max\n",
    "                maximum = max(ttsum)\n",
    "                for ii in ttsum:\n",
    "                    u += 1\n",
    "                    if ii == maximum:\n",
    "                        break\n",
    "                new_action = s.actions[u]\n",
    "                if old_action==new_action:\n",
    "                    check_action += 1\n",
    "                else:\n",
    "                    Policy[s] = new_action\n",
    "                    Vinit,_ = Evaluation(Policy,Vinit,states)\n",
    "    table = np.vstack((table,get_policy_actions_or_values(Policy),get_policy_actions_or_values(Vinit)))\n",
    "    if check_action == 10:\n",
    "        proof +=1\n",
    "    if proof == 2:\n",
    "        policy_stable = False\n",
    "\n",
    "table = pd.DataFrame(table)\n",
    "# table = table[-1:]\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last iteration above proves that the policy does not improve beyond that iteration. That means the final policy and state values are optimal policy and value functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
